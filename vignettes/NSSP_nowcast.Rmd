---
title: "Nowcasting NSSP data"
description: "A nowcasting example applied to data from NSSP"
authors: Kaitlyn Johnson, Emily Tyszka
output:
  bookdown::html_document2:
    fig_caption: yes
    code_folding: show
pkgdown:
  as_is: true
bibliography: library.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa-numeric-superscript-brackets.csl
link-citations: true
vignette: >
  %\VignetteIndexEntry{Nowcasting NSSP data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

In this vignette, we will show an example of how to clean and pre-process line-list patient-level data from the National Syndromic Surveillance (NSSP) dataset.
This dataset is entirely fake, it represents a small number of simulate records that mirror the NSSP update fields.
It was created to demonstrate how to take advantage of the timesteps embedded in the NSSP ESSENCE visits records dataset, which records the timing of diagnoses code updates related to each clinical encounter.
Diagnosis code may begin to be associated with the clinical encounter at different points in the patient or clinical processing journey (e.g. during admission, triage, after laboratory results are returned, or during coding for billing, etc.), and are captured by the syndromic surveillance system as they are entered into the electronic health record.
The difference between the visit date - when the patient registers in the emergency department- and the time of the diagnosis update pertaining to the diagnosis of interest, is used to compute a reporting delay for each patient.
Reporting delays can vary by a range of factors including the electronic health record system, facility, provide, season, time of day, or means of diagnosis.
These reporting delays will be used to estimate a reporting delay distribution.

In this near-real-time emergency department dataset, public health surveillance definitions have been developed for a range of public health concerns and these definitions typically rely on the presence of diagnosis code(s), specific free text captured in clinical notes, or a combination of these.
In additional, exclusion criteria are used in some definitions to improve the specificity of these definitions in some instances.

In this example, we will focus on syndromic surveillance definitions that rely solely on the presence of one or more diagnoses codes, as this means that patients tend to transition from a non-case to case only once (rather than reverting back to a non-case during the course of their clinical encounter).
Our nowcasting methods primarily focus on the former.

 We will walk through how to "clean" the line list NSSP dataset in order to obtain a long tidy dataframe containing the incident counts of "cases" of a particular syndromic surveillance definition by reference date (in this case the date the visit started) and report date (the date the patient's record was updated with the corresponding diagnosis).

Once we have shown how to generate data of this form from the line-list data, we will swap out the dataframe for a larger mock dataset formatted the same way, and demonstrate how to use `baselinenowcast` to generate a nowcast of the admissions.
We will start by using the diagnosis codes that define Acute Respiratory Infections (ARI), but any sets of diagnosis codes that define a syndrome could be used interchangeably.


## Load packages
```{r setup, message = FALSE, warning = FALSE, results = 'hide'}
# Load packages
library(baselinenowcast)
library(ggplot2)
library(stringr)
library(dplyr)
library(tidyr)
library(readr)
library(lubridate)
library(purrr)
# Set seed for reproducibility
set.seed(123)
```



# NSSP data cleaning

We will start by loading in the mock line-list dataset.
Our goal is to turn the line-list's time stamp and diagnosis update columns into first a long format with one row per update, then filter through the updates to find the first "hit" that corresponds to the diagnosis codes in the syndromic surveillance definition for ARI.
In this dataset, the first update time represents the date of reporting of the diagnosis, and the visit time represents the date in which the reference event, in this case the date of the visit, occurred.
We will then summarise the data in long format, obtaining a long tidy dataframe with the counts of admissions on each reference date (visit date) and report date (the first time the diagnosis occurred during the clinical encounter).


## Load in the line list data
This typically will be pulled using an API, but here we provide the `NSSP_line_list_raw` dataset as package data.
```{r}
NSSP_line_list_raw
```
<div class="alert alert-warning" role="alert">
<strong>Note:</strong> This dataset does not represent data from real patients, it is entirely simulated.
</div>


## Define the syndromic surveillance definition diagnosis codes.
Here we will list the diagnosis codes which correspond to ARI.
To be considered an ARI case, one or more of these codes must be reported.
```{r}
diagnoses_codes_defn <- c("U07", "R112")
```

## Expand the data so that each "event" has its own column
We will create two datasets which parse the characters in the columns `DischargeDiagnosisMDTUpdates` and `DischargeDiagnosisUpdates` , which contain a string listing the time stamp and diagnosis codes (respectively) of each "event" in the clinical encounter, formatted as:

 - `"{event number};YYYY-MM-DD HH:MM:SS;|{event number 2};YYYY-MM-DD HH:MM:SS;|" for "DischargeDiagnosisMDTUpdates"`

 - `"{event number};{diagnoses codes};|{event number 2}{diagnoses codes};|" for "DischargeDiagnosisUpdates"`

Later, we will merge the two datasets back together by the unique patient ID and the event numbers.

We will use `tidyr`'s `separate_wider_delim` to expand these entries, so that each "event" has its own column.
Since patients experience a different number of patient update "events", there will be missing values for patients not experiencing many events during their visit.
The columns will be named by the original column name + the event number, e.g. `DischargeDiagnosisMDTUpdates1`.

```{r}
NSSP_time_stamps_wide <- NSSP_line_list_raw |>
  separate_wider_delim(DischargeDiagnosisMDTUpdates,
    delim = "{", names_sep = "", too_few = "align_start"
  ) |>
  select(-DischargeDiagnosisUpdates)
head(NSSP_time_stamps_wide)
```
Repeat for the diagnoses codes column.
```{r}
NSSP_diagnoses_wide <- NSSP_line_list_raw |>
  separate_wider_delim(DischargeDiagnosisUpdates,
    delim = "{", names_sep = "", too_few = "align_start"
  ) |>
  select(-DischargeDiagnosisMDTUpdates)
```

Find the name of the last update column, and pivot the data from wide to long.
This creates a long tidy dataframe where each row is now an event.
We will create a unique event ID using the event number and the patient ID in the `C_Processed_BioSense_ID` column.
```{r}
lastcol_ts <- tail(colnames(select(
  NSSP_time_stamps_wide,
  matches("DischargeDiagnosisMDTUpdates")
)), 1)
NSSP_time_stamps_long <- gather(NSSP_time_stamps_wide,
  key = column_name,
  value = "time_stamp",
  DischargeDiagnosisMDTUpdates1:as.name(`lastcol_ts`),
  factor_key = TRUE
) |>
  mutate(
    event_id = paste(
      C_Processed_BioSense_ID,
      parse_number(as.character(column_name))
    )
  )
```

Perform the same procedure for the diagnoses dataset.
```{r}
lastcol_diag <- tail(colnames(select(
  NSSP_diagnoses_wide,
  matches("DischargeDiagnosisUpdates")
)), 1)
NSSP_diagnoses_long <- gather(NSSP_diagnoses_wide,
  key = column_name,
  value = "diagnoses_codes",
  DischargeDiagnosisUpdates1:as.name(`lastcol_diag`),
  factor_key = TRUE
) |>
  mutate(
    event_id = paste(
      C_Processed_BioSense_ID,
      parse_number(as.character(column_name))
    )
  )
```

Next, we will clean up the time stamps in the data so that the "`time_stamp`" column is formatted as `%Y-%m-%d %H:%M:%S`, and then we will filter out an events that are not present (updates are NAs).
```{r}
NSSP_time_stamps_long <-
  NSSP_time_stamps_long |>
  mutate(time_stamp = as.POSIXct(
    str_remove_all(
      str_remove(time_stamp, ".*\\}"),
      "[|;]+"
    ),
    format = "%Y-%m-%d %H:%M:%S",
    tz = "UTC"
  )) |>
  drop_na(time_stamp)
```

Clean up the diagnoses codes and remove the empty updates from the diagnoses dataset.
For these, we want to keep the semi-colons and just remove the numbers since
this information is stored in the event ID.
We will only use the event ID and the diagnoses codes, as this will be merged
back into the time stamped dataset.
```{r}
NSSP_diagnoses_long <-
  NSSP_diagnoses_long |>
  mutate(diagnoses_codes = str_remove(diagnoses_codes, ".*\\}")) |>
  filter(nzchar(diagnoses_codes, "")) |>
  drop_na() |>
  select(event_id, diagnoses_codes)
```


Merge together the time stamps of events and the diagnoses codes.
Filter to remove empty update
```{r}
df_all <- merge(NSSP_time_stamps_long,
  NSSP_diagnoses_long,
  by = "event_id"
) |>
  filter(diagnoses_codes != ";;|") # drops empty updates
```
Now we have a dataframe where each row is an event, with the patient's visit start date (`C_Visit_date_Time`), the patient ID (`C_Processed_BioSense_ID`), the diagnoses code at the event (`diagnoses_code`), and the time stamp of the event (`time_stamp`).

Next we will add a columns for the time from arrival to each updated diagnosis, in days.
```{r}
df_all <- df_all |>
  mutate(arrival_to_update_delay = as.numeric(difftime(
    as.POSIXct(time_stamp), as.POSIXct(C_Visit_Date_Time),
    units = "days"
  )))
```

Next, apply the query to identify when the patient first was diagnosed with ARI.
```{r}
ARI_updates <- df_all |>
  filter(map_lgl(diagnoses_codes, ~ any(str_detect(.x, diagnoses_codes_defn))))
```

Next, we will order these by the delay from visit to the diagnoses, and for each patient keep only the first update containing the ARI diagnoses code(s).
```{r}
first_ARI_diagnosis <- ARI_updates |>
  arrange(arrival_to_update_delay) |>
  group_by(C_Processed_BioSense_ID) |>
  slice(1)
```

Clean up for nowcasting.
```{r}
clean_line_list <- first_ARI_diagnosis |>
  mutate(
    reference_date = as.Date(C_Visit_Date_Time),
    report_date = as.Date(time_stamp)
  ) |>
  ungroup() |>
  select(reference_date, report_date)
head(clean_line_list)
```

Get the counts by reference and report date and compute the delay.
```{r}
count_df <- clean_line_list |>
  group_by(reference_date, report_date) |>
  summarise(count = n()) |>
  mutate(delay = as.integer(report_date - reference_date))
```
Looking at this data, we can see that there is one case where there is a negative delay, which indicates that the time stamp of the diagnosis update was recorded before the start of the visit.
This is likely due to a data entry error, we will choose to exclude all negative values delays in our dataset.

Filter out negative delays.
```{r}
count_df <- filter(count_df, delay >= 0)
head(count_df)
```

We have now generated data in the format that we need to use the `baslinenowcast` package, which requires a long tidy dataframe with incident case counts indexed by reference date and report date.
For demonstration purposes, we will now swap out the data from the mock NSSP line-list data with a larger mock dataset.

# Pre-processing of larger mock dataset

We'll start by loading in the mock longer dataset, which is also provided as package data
```{r}
mock_long_df
```
You can see that this larger mock dataset has the same format as the one we generated from the line list NSSP data -- with columns for reference date, report date, and counts.

## Exploratory data analysis to identify an appropriate maximum delay

We will start by computing the reporting delay and performing an exploratory data analysis of the delays in this dataset.
<details><summary>Click to expand code to create plots of the delay distributions </summary>
```{r}
long_df <- mock_long_df |>
  mutate(delay = as.integer(report_date - reference_date))

delay_df_t <- long_df |>
  group_by(reference_date) |>
  summarise(mean_delay = sum(count * delay) / sum(count))

delay_summary <- long_df |>
  mutate(mean_delay_overall = sum(count * delay) / sum(count))

avg_delays <- long_df |>
  group_by(delay) |>
  summarise(pmf = sum(count) / sum(long_df$count)) |>
  mutate(cdf = cumsum(pmf))

delay_t <- ggplot(delay_df_t) +
  geom_line(aes(
    x = reference_date,
    y = mean_delay
  )) +
  geom_line(
    data = delay_summary,
    aes(
      x = reference_date,
      y = mean_delay_overall
    ),
    linetype = "dashed"
  ) +
  xlab("") +
  ylab("Mean delay") +
  theme_bw()

cdf_delay <- ggplot(avg_delays) +
  geom_line(aes(x = delay, y = cdf)) +
  geom_hline(aes(yintercept = 0.95), linetype = "dashed") +
  theme_bw()
```
</details>
```{r}
delay_t
cdf_delay
```
Based on this figure, we can set the maximum delay to be 25 days as this is where 95% of the cases appear to have been reported.
We will also set the nowcast date as 30 days before the maximum reference date in the dataset, May 5th, 2026.

```{r}
max_delay <- 25
nowcast_date <- max(long_df$reference_date) - days(30)
```

## Format for `baselinenowcast`
We'll start by generating a reporting triangle from the `mock_long_df`, after we've removed all of the reports from after the nowcast date.
We will need to fill in all the possible reference dates and report dates with delays up until the maximum delay.
We'll do this by first making a "spine" of all of the date combinations we need, and then joining the dataset to that spine.
As a final step, we'll pivot filter out the report date column and pivot from wide to long format.


```{r}
training_df <- long_df |>
  filter(report_date <= nowcast_date) |>
  mutate(delay = as.integer(report_date - reference_date)) |>
  filter(delay <= max_delay) |>
  select(reference_date, report_date, count)

rep_tri_df <- expand_grid(
  reference_date = seq(min(long_df$reference_date),
    nowcast_date,
    by = "day"
  )
) |>
  rowwise() |>
  reframe(
    reference_date = reference_date,
    report_date = seq(reference_date, min(
      reference_date + max_delay,
      nowcast_date
    ),
    by = "day"
    )
  ) |>
  mutate(delay = as.integer(report_date - reference_date)) |>
  left_join(training_df,
    by = c("reference_date", "report_date")
  ) |>
  replace_na(list(count = 0)) |>
  select(-report_date) |>
  pivot_wider(
    names_from = delay,
    values_from = count
  )
head(rep_tri_df)
```
## Specify the `baselinenowcast` model

We will need to tell the model how many reference times to use for delay estimation and uncertainty estimation.
We recommend choosing these as functions of the maximum delay.
```{r}
n_ref_times_used <- 3 * max_delay
n_history_delay <- floor(n_ref_times_used / 2)
n_retrospective_nowcasts <- n_ref_times_used - n_history_delay
```

# Run the `baselinenowcast` workflow
Here we demonstrate the workflow using the low-level functions. For more details, see the [Getting Started](baselinenowcast.html) vignette.
```{r}
rep_tri <- rep_tri_df |>
  select(-reference_date) |>
  as.matrix()

# Delay estimation
delay_pmf <- estimate_delay(
  reporting_triangle = rep_tri,
  max_delay = max_delay,
  n = n_history_delay
)
# Point nowcast generation
pt_nowcast_matrix <- apply_delay(
  reporting_triangle = rep_tri,
  delay_pmf = delay_pmf
)
# Unertainty estimation
trunc_rep_tris <- truncate_triangles(rep_tri,
  n = n_retrospective_nowcasts
)
retro_rep_tris <- construct_triangles(trunc_rep_tris)
pt_nowcasts <- fill_triangles(retro_rep_tris,
  n = n_history_delay
)
disp <- estimate_uncertainty(
  point_nowcast_matrices = pt_nowcasts,
  truncated_reporting_triangles = trunc_rep_tris,
  retro_reporting_triangles = retro_rep_tris,
  n = n_retrospective_nowcasts
)

nowcast_draws_df <- sample_nowcasts(
  point_nowcast_matrix = pt_nowcast_matrix,
  reporting_triangle = rep_tri,
  uncertainty_params = disp,
  draws = 100
)
head(nowcast_draws_df)
```

# Summarise and plot the nowcast
Now that we have a dataframe of probabilistic nowcast draws for all reference times in our original reporting triangle, we can summarise the median and 50th and 95th quantiles.
```{r}
nowcast_summary_df <-
  nowcast_draws_df |>
  group_by(time) |>
  summarise(
    median = median(pred_count),
    q50th_lb = quantile(pred_count, 0.25),
    q50th_ub = quantile(pred_count, 0.75),
    q95th_lb = quantile(pred_count, 0.025),
    q95th_ub = quantile(pred_count, 0.975)
  )
```

Next, we will join the initially reported data and the final evaluation data, both summarised by reference time.
```{r}
training_df_by_ref_date <- training_df |>
  filter(report_date <= nowcast_date) |>
  group_by(reference_date) |>
  summarise(initial_count = sum(count)) |>
  mutate(time = row_number())

eval_data <- long_df |>
  filter(
    delay <= max_delay,
    reference_date <= nowcast_date
  ) |>
  group_by(reference_date) |>
  summarise(final_count = sum(count)) |>
  mutate(time = row_number()) |>
  select(time, final_count)

nowcast_w_data <- nowcast_summary_df |>
  left_join(training_df_by_ref_date,
    by = "time"
  ) |>
  left_join(eval_data,
    by = "time"
  )
head(nowcast_w_data)
```

# Plot against later observed "final" data
Next we will make a plot of the nowcast
```{r}
combined_data <- nowcast_w_data |>
  select(reference_date, initial_count, final_count) |>
  distinct() |>
  pivot_longer(
    cols = c(initial_count, final_count),
    names_to = "type",
    values_to = "count"
  ) |>
  mutate(type = case_when(
    type == "initial_count" ~ "Initially observed data",
    type == "final_count" ~ "Final observed data"
  )) |>
  filter(reference_date >= nowcast_date - days(60))
```
<details><summary>Click to expand code to create the plot of the probabilistic nowcast</summary>
```{r}
nowcast_data_recent <- nowcast_w_data |>
  filter(reference_date >= nowcast_date - days(60))

plot_prob_nowcast <- ggplot(nowcast_data_recent) +
  geom_line(
    aes(
      x = reference_date, y = median
    ),
    color = "gray"
  ) +
  geom_ribbon(
    aes(
      x = reference_date,
      ymin = q50th_ub, ymax = q50th_lb
    ),
    alpha = 0.5,
    fill = "gray"
  ) +
  geom_ribbon(
    aes(
      x = reference_date,
      ymin = q95th_ub, ymax = q95th_lb
    ),
    alpha = 0.5,
    fill = "gray"
  ) +
  # Add observed data and final data once
  geom_line(
    data = combined_data,
    aes(
      x = reference_date,
      y = count,
      color = type
    )
  ) +
  theme_bw() +
  scale_color_manual(
    values = c(
      "Initially observed data" = "darkred",
      "Final observed data" = "black"
    ),
    name = ""
  ) +
  xlab("Date of ED visit") +
  ylab("Number of ARI cases") +
  theme(legend.position = "bottom") +
  ggtitle("Comparison of cases of ARI as of the nowcast date, later observed,
          \n and generated as a probabilistic nowcast")
```
</details>
```{r}
plot_prob_nowcast
```
