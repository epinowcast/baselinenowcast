---
title: "Nowcasting syndromic surveillance system data: a case study applied to the U.S. National Syndromic Surveillance Program (NSSP) data"
description: "A nowcasting example applied to data from NSSP"
output:
  bookdown::html_document2:
    fig_caption: yes
    code_folding: show
pkgdown:
  as_is: true
bibliography: library.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa-numeric-superscript-brackets.csl
link-citations: true
vignette: >
  %\VignetteIndexEntry{Nowcasting syndromic surveillance system data: a case study applied to the U.S. National Syndromic Surveillance Program (NSSP) data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

Syndromic surveillance data are used by public health departments to understand trends in clinical encounters from electronic health records.
When these datasets are analysed in real-time, delays from the initial visit to updated diagnoses codes results in right-truncation of counts of the primary event, i.e. the patient visit, leading to systematic downward bias in the most recent data.
Nowcasting uses the previously observed reporting delays to correct for this downward bias and estimate the eventual, final observed cases, which provides an estimate of the trend in cases in real-time.

## About syndromic surveillance system data

Syndromic surveillance system data contains information at the visit-level on the timing and nature of the patient's clinical interactions.
In this case study, we will use synthetic data from the United States' [National Syndromic Surveillance Program](https://www.cdc.gov/nssp/index.html) (NSSP) dataset available for participating jurisdictions through the CDC's ESSENCE platform, to nowcast cases of Broad Acute Respiratory Incidence (BAR).
The NSSP Emergency Department (ED) visit dataset is a dataset widely used by public health departments in the United States, representing many but not all counties in the country.
The dataset contains information at the visit-level about each clinical encounter recorded in the electronic health record system during the patient's hospital stay.
Clinical encounters may begin to be associated with diagnoses codes at different points in the patient or clinical processing journey (e.g. during registration, triage, clinical encounter, after laboratory results are returned, or during coding for billing, etc.), and are captured through update messages to the syndromic surveillance system once they are entered into a facility's electronic health record.
The difference between the visit date - when the patient registers in the emergency department- and the time of the diagnosis update pertaining to the diagnosis of interest, is used to compute a reporting delay for each patient.
Reporting delays for diagnoses of interest can vary by a range of factors including by pathogen/syndrome, season, time of day or week, means of diagnosis, the electronic health record system, or treating facility.
In this vignette, we will demonstrate how to use syndromic surveillance system data, particularly the timestamped data captured in each NSSP ESSENCE visit record, to produce nowcasts of cases of a particular diagnosis of interest.
Note that all visits that originate in the emergency department are used for this analysis, regardless of eventual inpatient admission.


## Load packages

We use the `baselinenowcast` package, `dplyr`, and `tidyr` for data manipulation, `readr` for loading in data, `stringr` for parsing text data, `lubridate` for formatting dates, `ggplot2` for plotting, and `purrr` for mapping diagnoses codes to text fields in the data.
```{r setup, message = FALSE, warning = FALSE, results = 'hide'}
# Load packages
library(baselinenowcast)
library(dplyr)
library(tidyr)
library(readr)
library(stringr)
library(lubridate)
library(ggplot2)
library(purrr)
# Set seed for reproducibility
set.seed(123)
```



# NSSP data pre-processing

We will walk through how to preprocess the line list NSSP data details dataset in order to obtain a [long tidy dataframe](https://r4ds.had.co.nz/tidy-data.html) containing the incident counts of "cases" of a particular syndrome by reference date (in this instance the date the visit started) and report date (the date the patient's record was updated with the corresponding diagnosis of interest).

## Load in the line list data

This typically will be pulled using an API, but here we provide the `syn_nssp_line_list` dataset as package data.
```{r}
syn_nssp_line_list
```
<div class="alert alert-warning" role="alert">
<strong>Note:</strong> This dataset does not represent data from real patients, it is entirely synthetic and designed to mirror the NSSP update fields, which records the timing of a clinical encounter and the corresponding diagnoses code, if any.
</div>


## Define the "syndrome" definition

In this near-real-time emergency department dataset, public health surveillance, or "syndrome" definitions have been developed for a range of public health concerns. These definitions typically rely on the presence of diagnosis code(s), specific free text captured in clinical notes, or a combination of these.
In some instances, exclusion criteria are incorporated into these definitions to improve their specificity.

In this example, we will focus on syndromic surveillance definitions that rely solely on the presence of one or more diagnoses codes, as this means that patients tend to transition from a non-case to case only once (rather than reverting back to a non-case during the course of their clinical encounter).
Our nowcasting methods primarily focus on the former use-case, though extensions to be able to account for downward revisions as cases transition from case to non-case, are planned for future work.

Here we will list the diagnosis codes which correspond to Broad Acute Respiratory, but any sets of diagnosis codes that define a syndrome could be used interchangeably.
To be considered a Broad Acute Respiratory case, one or more of these codes must be reported.
```{r}
diagnoses_codes_defn <- c("A22.1", "A221", "A37", "A48.1", "A481", "B25.0", "B250", "B34.2", "B34.9", "B342", "B349", "B44.0", "B44.9", "B440", "B449", "B44.81", "B4481", "B97.2", "B97.4", "B972", "B974", "J00", "J01", "J02", "J03", "J04", "J05", "J06", "J09", "J10", "J11", "J12", "J13", "J14", "J15", "J16", "J17", "J18", "J20", "J21", "J22", "J39.8", "J398", "J40", "J47.9", "J479", "J80", "J85.1", "J851", "J95.821", "J95821", "J96.0", "J96.00", "J9600", "J96.01", "J9601", "J96.02", "J9602", "J96.2", "J960", "J962", "J96.20", "J9620", "J96.21", "J9621", "J9622", "J96.22", "J96.91", "J9691", "J98.8", "J988", "R05", "R06.03", "R0603", "R09.02", "R0902", "R09.2", "R092", "R43.0", "R43.1", "R43.2", "R430", "R431", "R432", "U07.1", "U07.2", "U071", "U072", "022.1", "0221", "034.0", "0340", "041.5", "0415", "041.81", "04181", "079.1", "079.2", "079.3", "079.6", "0791", "0792", "0793", "0796", "079.82", "079.89", "07982", "07989", "079.99", "07999", "117.3", "1173", "460", "461", "462", "463", "464", "465", "466", "461.", "461", "461.", "464.", "465.", "466.", "461", "464", "465", "466", "478.9", "4789", "480.", "482.", "483.", "484.", "487.", "488.", "480", "481", "482", "483", "484", "485", "486", "487", "488", "490", "494.1", "4941", "517.1", "5171", "518.51", "518.53", "51851", "51853", "518.6", "5186", "518.81", "518.82", "518.84", "51881", "51882", "51884", "519.8", "5198", "073.0", "0730", "781.1", "7811", "786.2", "7862", "799.02", "79902", "799.1", "7991", "033", "033.", "033", "780.60", "78060") # nolint
```

## Expand the data so that each "event" has its own column

First we will pivot the line-list's time stamp and diagnosis update columns into a long format with one row per update.

We will create two datasets which parse the characters in the columns `DischargeDiagnosisMDTUpdates` and `DischargeDiagnosisUpdates` , which contain a string listing the time stamp and diagnosis codes (respectively) of each "event" in the clinical encounter, formatted as:

 - `{event number};YYYY-MM-DD HH:MM:SS;|{event number 2};YYYY-MM-DD HH:MM:SS;|` for `DischargeDiagnosisMDTUpdates`

 - `{event number};{diagnoses codes};|{event number 2}{diagnoses codes};|` for `DischargeDiagnosisUpdates`

The timestamp records the timing of diagnosis code updates related to each clinical encounter, capturing the point in time when each new code became available within the NSSP system.
Later, we will merge the two datasets back together by the unique patient ID and the event number, so that we can associate each set of diagnoses codes with a timestamp.

We will use `tidyr::separate_wider_delim()` to expand these entries, so that each "event" has its own column.
Since patients experience a different number of patient update "events", there will be missing values for patients not experiencing many events during their visit.
The columns will be named by the original column name + the event number, e.g. `DischargeDiagnosisMDTUpdates1`.
```{r}
syn_nssp_time_stamps_wide <- syn_nssp_line_list |>
  separate_wider_delim(DischargeDiagnosisMDTUpdates,
    delim = "{", names_sep = "", too_few = "align_start"
  ) |>
  select(-DischargeDiagnosisUpdates)
head(syn_nssp_time_stamps_wide)
```

Repeat for the diagnoses codes column.
```{r}
syn_nssp_diagnoses_wide <- syn_nssp_line_list |>
  separate_wider_delim(DischargeDiagnosisUpdates,
    delim = "{", names_sep = "", too_few = "align_start"
  ) |>
  select(-DischargeDiagnosisMDTUpdates)
```

Find the name of the last update column, and pivot the data from wide to long.
This creates a long tidy dataframe where each row is now an event.
We will create a unique event ID using the event number and the patient ID in the `C_Processed_BioSense_ID` column.
```{r}
lastcol_ts <- tail(colnames(select(
  syn_nssp_time_stamps_wide,
  matches("DischargeDiagnosisMDTUpdates")
)), 1)
syn_nssp_time_stamps_long <- syn_nssp_time_stamps_wide |>
  pivot_longer(
    cols = starts_with("DischargeDiagnosisMDTUpdates"),
    names_to = "column_name",
    values_to = "time_stamp",
    values_drop_na = FALSE
  ) |>
  mutate(
    event_id = paste(
      C_Processed_BioSense_ID,
      parse_number(as.character(column_name))
    )
  )
```

Perform the same procedure for the diagnoses dataset.
```{r}
lastcol_diag <- tail(colnames(select(
  syn_nssp_diagnoses_wide,
  matches("DischargeDiagnosisUpdates")
)), 1)
syn_nssp_diagnoses_long <- syn_nssp_diagnoses_wide |>
  pivot_longer(
    cols = starts_with("DischargeDiagnosisUpdates"),
    names_to = "column_name",
    values_to = "diagnoses_codes",
    values_drop_na = FALSE
  ) |>
  mutate(
    event_id = paste(
      C_Processed_BioSense_ID,
      parse_number(as.character(column_name))
    )
  )
```

Next, we will clean up the time stamps in the data so that the `time_stamp` column is formatted as `%Y-%m-%d %H:%M:%S`, and then we will filter out an events that are not present (updates are NAs).
```{r}
syn_nssp_time_stamps_long <-
  syn_nssp_time_stamps_long |>
  mutate(time_stamp = as.POSIXct(
    str_remove_all(
      str_remove(time_stamp, ".*\\}"),
      "[|;]+"
    ),
    format = "%Y-%m-%d %H:%M:%S",
    tz = "UTC"
  )) |>
  drop_na(time_stamp)
```

Clean up the diagnoses codes and remove the empty updates from the diagnoses dataset.
For these, we want to keep the semi-colons and just remove the numbers since
this information is stored in the event ID.
We will only use the event ID and the diagnoses codes, as this will be merged
back into the time stamped dataset.
```{r}
syn_nssp_diagnoses_long <-
  syn_nssp_diagnoses_long |>
  mutate(diagnoses_codes = str_remove(diagnoses_codes, ".*\\}")) |>
  filter(nzchar(diagnoses_codes)) |>
  drop_na() |>
  select(event_id, diagnoses_codes)
```


Merge together the time stamps of events and the diagnoses codes.
Filter to remove empty updates.
```{r}
nssp_merged <- merge(syn_nssp_time_stamps_long,
  syn_nssp_diagnoses_long,
  by = "event_id"
) |>
  filter(diagnoses_codes != ";;|") # drops empty updates
```
Now we have a dataframe where each row is an event, with the patient's visit start date (`C_Visit_date_Time`), the patient ID (`C_Processed_BioSense_ID`), the diagnoses code at the event (`diagnoses_code`), and the time stamp of the event (`time_stamp`).

Next we will add a column for the time from arrival to each updated diagnosis, in days.
```{r}
nssp_updates <- nssp_merged |>
  mutate(arrival_to_update_delay = as.numeric(difftime(
    as.POSIXct(time_stamp), as.POSIXct(C_Visit_Date_Time),
    units = "days"
  )))
```

We next filter through the updates to find the first "hit" that corresponds to the diagnosis codes in the syndromic surveillance definition for ARI.
```{r}
ARI_updates <- nssp_updates |>
  filter(map_lgl(diagnoses_codes, ~ any(str_detect(.x, diagnoses_codes_defn))))
```

Next, we will order these by the delay from visit to the diagnoses, and for each patient keep only the first update containing the ARI diagnoses code(s).
```{r}
first_ARI_diagnosis <- ARI_updates |>
  arrange(arrival_to_update_delay) |>
  group_by(C_Processed_BioSense_ID) |>
  slice(1)
```

Label the visit start date, `C_Visit_Date_Time`, as the reference date,`reference_date` and `time_stamp` as the report date, `report_date` and remove the other column names that are no longer needed, as each row now represents a case.
```{r}
clean_line_list <- first_ARI_diagnosis |>
  mutate(
    reference_date = as.Date(C_Visit_Date_Time),
    report_date = as.Date(time_stamp)
  ) |>
  ungroup()
head(clean_line_list)
```

For nowcasting, we want to compute the number of incident cases indexed by reference and report date, so we can aggregate by reference and report date and compute the delay distribution.
```{r}
count_df_raw <- clean_line_list |>
  group_by(reference_date, report_date) |>
  summarise(count = n()) |>
  mutate(delay = as.integer(report_date - reference_date))
```
Looking at this data, we can see that there is one case where there is a negative delay, which indicates that the time stamp of the diagnosis update was recorded before the start of the visit.
This is likely due to a data entry error.
We could choose to code these as having 0 delay, but if they are indeed erroneous this might bias our delay distribution estimation towards zero when they are equally likely to have had longer delays.
We choose to exclude all negative valued delays.

Filter out negative delays.
```{r}
count_df <- filter(count_df_raw, delay >= 0)
head(count_df)
```

We have now generated data in the format that we need to use the `baselinenowcast` package, which requires a [long tidy dataframe](https://r4ds.had.co.nz/tidy-data.html) with incident case counts indexed by reference date and report date.
For demonstration purposes, we will now swap out the data from the simulated NSSP line-list data with a larger synthetic dataset.

# Pre-processing of larger synthetic dataset

We'll start by loading in the synthetic reporting triangle dataframe, which is also provided as package data.
```{r}
syn_nssp_df
```
<div class="alert alert-warning" role="alert">
<strong>Note:</strong> This dataset represents synthetic data on the number of incident cases indexed by reference date and report date. It was generated to approximately mirror trends in ARI cases during a season without using any real data. See `?syn_nssp_df` for full documentation.
</div>

You can see that this larger synthetic dataset has the same format as the one we generated from the line list NSSP data -- with columns for reference date, report date, and counts.

## Exploratory data analysis to identify an appropriate maximum delay

To produce a nowcast with `baselinenowcast`, we will first want to perform an exploratory data analysis to visualize trends in the delay distribution.
This will help us choose a maximum delay and specify the number of reference times we want to use for delay and uncertainty estimation.
<details><summary>Click to expand code to create plots of the delay distributions </summary>
```{r}
long_df <- syn_nssp_df |>
  mutate(delay = as.integer(report_date - reference_date))

delay_df_t <- long_df |>
  group_by(reference_date) |>
  summarise(mean_delay = sum(count * delay) / sum(count))

delay_summary <- long_df |>
  mutate(mean_delay_overall = sum(count * delay) / sum(count))

avg_delays <- long_df |>
  group_by(delay) |>
  summarise(pmf = sum(count) / sum(long_df$count)) |>
  mutate(cdf = cumsum(pmf))

delay_t <- ggplot(delay_df_t) +
  geom_line(aes(
    x = reference_date,
    y = mean_delay
  )) +
  geom_line(
    data = delay_summary,
    aes(
      x = reference_date,
      y = mean_delay_overall
    ),
    linetype = "dashed"
  ) +
  xlab("") +
  ylab("Mean delay") +
  theme_bw()

cdf_delay <- ggplot(avg_delays) +
  geom_line(aes(x = delay, y = cdf)) +
  geom_hline(aes(yintercept = 0.95), linetype = "dashed") +
  theme_bw()
```
</details>
```{r}
cdf_delay
delay_t
```

Based on this figure, we can set the maximum delay to be 25 days as this is where 95% of the cases appear to have been reported.
We will also set the nowcast date as 30 days before the maximum reference date in the dataset, May 5th, 2026.
This is so that we can use the later observed data, beyond the 25 days maximum delay, to evaluate our nowcast against.
```{r}
max_delay <- 25
nowcast_date <- max(long_df$reference_date) - days(30)
```


## Format for `baselinenowcast`

We'll start by generating a reporting triangle (see the [mathematical model](model_definition.html) vignette for more details) from the `syn_nssp_df`, after we've removed all of the reports from after the nowcast date.

In order to produce a reporting triangle, we need to obtain the number of incident case counts at all reference dates and for all corresponding report dates up until the maximum delay.
We'll do this by first making a "spine" of all of the date combinations we need, and then joining the dataset to that spine.
As a final step, we'll pivot filter out the report date column and pivot from wide to long format.
```{r}
training_df <- long_df |>
  filter(report_date <= nowcast_date) |>
  mutate(delay = as.integer(report_date - reference_date)) |>
  filter(delay <= max_delay) |>
  select(reference_date, report_date, count)

rep_tri_df <- expand_grid(
  reference_date = seq(min(long_df$reference_date),
    nowcast_date,
    by = "day"
  )
) |>
  rowwise() |>
  reframe(
    reference_date = reference_date,
    report_date = seq(reference_date, min(
      reference_date + max_delay,
      nowcast_date
    ),
    by = "day"
    )
  ) |>
  mutate(delay = as.integer(report_date - reference_date)) |>
  left_join(training_df,
    by = c("reference_date", "report_date")
  ) |>
  replace_na(list(count = 0)) |>
  select(-report_date) |>
  pivot_wider(
    names_from = delay,
    values_from = count
  )
head(rep_tri_df)
```

As a final pre-processing step, we'll format the reporting triangle as just a matrix, for compatibility with the low-level functions in `baselinenowcast`.
```{r}
rep_tri <- rep_tri_df |>
  select(-reference_date) |>
  as.matrix()
```

## Specify the `baselinenowcast` model

We will use the helper function `allocate_reference_times()` to determine how many reference times to use for delay estimation and uncertainty estimation, using the package defaults.
See the "Default Settings" section of the [mathematical model](model_definition.html) vignette for more details.
```{r}
ref_time_allocation <- allocate_reference_times(rep_tri)
n_history_delay <- ref_time_allocation$n_history_delay
n_retrospective_nowcasts <- ref_time_allocation$n_retrospective_nowcasts
```

# Run the `baselinenowcast` workflow

First, we generate the point estimate of the nowcast as a "filled in" reporting triangle.
See the sections "Estimating the delay distribution from a reporting triangle" and "Point nowcast generation" of the [mathematical model](model_definition.html) for more details.
```{r}
pt_nowcast_matrix <- estimate_and_apply_delay(rep_tri, n = n_history_delay)
```

Next, we estimate and apply uncertainty by generating retrospective nowcasts and comparing against later observed data.
See the section "Uncertainty quantification" of the [mathematical_model](model_definition.html) for more details on assumptions being made here and a further description of how the uncertainty is quantified using past nowcast errors.
```{r}
nowcast_draws_df <- estimate_and_apply_uncertainty(
  pt_nowcast_matrix,
  rep_tri,
  n_history_delay = n_history_delay,
  n_retrospective_nowcasts = n_retrospective_nowcasts
)

head(nowcast_draws_df)
```

# Summarise and plot the nowcast

Now that we have a dataframe of probabilistic nowcast draws, which represent our predicted "final" case count at each reference time, we can summarise them to compute the 50th and 95th percent prediction intervals, which are used to communicate uncertainty in the nowcast.
```{r}
nowcast_summary_df <-
  nowcast_draws_df |>
  group_by(time) |>
  summarise(
    median = median(pred_count),
    q50th_lb = quantile(pred_count, 0.25),
    q50th_ub = quantile(pred_count, 0.75),
    q95th_lb = quantile(pred_count, 0.025),
    q95th_ub = quantile(pred_count, 0.975)
  )
```

Next, we will join the initially reported cases and the held out, true final cases to our probabilistic nowcast.
The final cases will be used to evaluate the performance of our nowcast given the right-truncated initial reports.
```{r}
training_df_by_ref_date <- training_df |>
  filter(report_date <= nowcast_date) |>
  group_by(reference_date) |>
  summarise(initial_count = sum(count)) |>
  mutate(time = row_number())

eval_data <- long_df |>
  filter(
    delay <= max_delay,
    reference_date <= nowcast_date
  ) |>
  group_by(reference_date) |>
  summarise(final_count = sum(count)) |>
  mutate(time = row_number()) |>
  select(time, final_count)

nowcast_w_data <- nowcast_summary_df |>
  left_join(training_df_by_ref_date,
    by = "time"
  ) |>
  left_join(eval_data,
    by = "time"
  )
head(nowcast_w_data)
```

## Plot against later observed "final" data
Next we will make a plot of the nowcasted "final" cases compared to the initially reported, right-truncated cases and the true "final" reports.
```{r}
combined_data <- nowcast_w_data |>
  select(reference_date, initial_count, final_count) |>
  distinct() |>
  pivot_longer(
    cols = c(initial_count, final_count),
    names_to = "type",
    values_to = "count"
  ) |>
  mutate(type = case_when(
    type == "initial_count" ~ "Initially observed data",
    type == "final_count" ~ "Final observed data"
  )) |>
  filter(reference_date >= nowcast_date - days(60))
```
<details><summary>Click to expand code to create the plot of the probabilistic nowcast</summary>
```{r}
nowcast_data_recent <- nowcast_w_data |>
  filter(reference_date >= nowcast_date - days(60))

plot_prob_nowcast <- ggplot(nowcast_data_recent) +
  geom_line(
    aes(
      x = reference_date, y = median
    ),
    color = "gray"
  ) +
  geom_ribbon(
    aes(
      x = reference_date,
      ymin = q50th_lb, ymax = q50th_ub
    ),
    alpha = 0.5,
    fill = "gray"
  ) +
  geom_ribbon(
    aes(
      x = reference_date,
      ymin = q95th_lb, ymax = q95th_ub
    ),
    alpha = 0.5,
    fill = "gray"
  ) +
  # Add observed data and final data once
  geom_line(
    data = combined_data,
    aes(
      x = reference_date,
      y = count,
      color = type
    )
  ) +
  theme_bw() +
  scale_color_manual(
    values = c(
      "Initially observed data" = "darkred",
      "Final observed data" = "black"
    ),
    name = ""
  ) +
  xlab("Date of ED visit") +
  ylab("Number of ARI cases") +
  theme(legend.position = "bottom") +
  ggtitle("Comparison of cases of ARI as of the nowcast date, later observed,\n and generated as a probabilistic nowcast") # nolint
```
</details>
```{r}
plot_prob_nowcast
```

# Summary

In this vignette we used `baselinenowcast ` to nowcast cases of ARI starting from synthetic syndromic surveillance system data designed to mirror the U.S. NSSP dataset.
We walked through the process of defining a syndromic surveillance definition using a list of diagnoses codes, and using the time stamps of updates in an electronic health record in the NSSP dataset to create a count of the number of cases of a specific definition indexed by the date of their visit and the date at which the patient's diagnoses was recorded into the surveillance system.
From there we used the `baselinenowcast` workflow to estimate a delay distribution, apply it to generate a point nowcast, and estimate and apply uncertainty to generate probabilistic nowcasts.
As a final step, we compared our nowcasts of the eventual final observed case counts to what we later observed and the right-truncated initial reports.

Next steps include scoring the nowcasts we generated using proper scoring rules such as the weighted interval score or the continuous ranked probability score and found the coverage metrics.
Since the `baselinenowcast` method is very fast, we encourage users to test out on a range of nowcast dates, and consider other model specifications such as weekday specific delay estimation or variations in the amount of training data used for delay and uncertainty estimation.
For more details on the performance of these method specifications applied to data from COVID-19 in Germany and norovirus cases in England, see our latest [pre-print](https://www.medrxiv.org/content/10.1101/2025.08.14.25333653v1.full).

We encourage users to explore model specifications tailored to their data and scenario, which can be done most readily via the modular low level function interface described in detail in the [Getting Started](baselinenowcast.html) vignette.
